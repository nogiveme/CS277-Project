\section{Timetable and Workload Division}
\label{sec:Timetable and workload division}
The project will be conducted from \textbf{October 2024 to January 2025}, following the institutional schedule for proposal, interim, and final submissions.
It is organized into five major stages, each with distinct objectives and responsibilities distributed among the four team members (A--D).
The division of work ensures balanced contribution across literature review, data preparation, modeling, evaluation, and reporting.

\subsection{Stage 1: Literature Review and Project Planning (Oct 9 -- Oct 20)}
This stage focuses on developing a comprehensive understanding of financial risk prediction, multi-modal data fusion, and Transformer-based time-series analysis.
The team will define the project scope, objectives, and expected outcomes while finalizing data sources and workflow for subsequent stages.

\textbf{Member responsibilities:}
\begin{itemize}
    \item \textbf{A:} Research background, theoretical framework, and risk classification (credit, market, liquidity).
    \item \textbf{B:} Literature review on deep learning and Transformer architectures.
    \item \textbf{C:} Drafting project objectives, defining scope, and formulating the problem statement.
    \item \textbf{D:} Proposal formatting and coordination of the presentation.
\end{itemize}

\subsection{Stage 2: Data Collection and Preprocessing (Oct 21 -- Nov 23)}
The focus is on acquiring and preparing the data. Both dynamic (daily K-line, returns, volatility, liquidity indices) and static (financial statements, leverage ratios, macroeconomic indicators) data will be collected, cleaned, normalized, and aligned. Exploratory analysis will be conducted to extract informative features.

\textbf{Member responsibilities:}
\begin{itemize}
    \item \textbf{A:} Dynamic data acquisition and preprocessing.
    \item \textbf{B:} Static data collection and integration.
    \item \textbf{C:} Data normalization, imputation, and feature engineering.
    \item \textbf{D:} Exploratory data visualization and documentation.
\end{itemize}

\subsection{Stage 3: Model Development and Interim Report (Nov 24 -- Dec 2)}
During this phase, the multi-modal Transformer framework will be implemented, combining temporal and static encoders via a cross-attention mechanism. The team will complete preliminary training, evaluate early results, and prepare the interim report and presentation.

\textbf{Member responsibilities:}
\begin{itemize}
    \item \textbf{A:} Transformer encoder and attention fusion implementation.
    \item \textbf{B:} Integration of preprocessing modules and debugging.
    \item \textbf{C:} Experimental setup and baseline model comparison (LSTM, GRU).
    \item \textbf{D:} Report writing and interim presentation preparation.
\end{itemize}

\subsection{Stage 4: Model Optimization and Evaluation (Dec 3 -- Dec 30)}
This stage involves hyperparameter tuning, model robustness tests, and interpretability analysis. Performance will be evaluated using standard regression and classification metrics, and comparative experiments will be conducted against baseline models.

\textbf{Member responsibilities:}
\begin{itemize}
    \item \textbf{A:} Hyperparameter tuning and robustness validation.
    \item \textbf{B:} Baseline benchmarking and error analysis.
    \item \textbf{C:} Interpretability analysis (attention visualization, SHAP analysis).
    \item \textbf{D:} Report writing and presentation coordination.
\end{itemize}

\subsection{Stage 5: Paper Writing and Submission (Jan 1 -- Jan 9)}
In the final stage, the team will prepare the academic paper or extended report summarizing the findings, finalize figures and references, and submit the complete work for evaluation or publication.

\textbf{Member responsibilities:}
\begin{itemize}
    \item \textbf{A:} Writing of introduction and related work.
    \item \textbf{B:} Methodology and results sections.
    \item \textbf{C:} Figures, tables, and supplementary content.
    \item \textbf{D:} Editing, proofreading, and submission coordination.
\end{itemize}
